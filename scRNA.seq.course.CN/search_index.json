[
["index.html", "scRNA-seq数据分析 1 说在前面的话 1.1 关于本项目 1.2 视频 1.3 GitHub 1.4 Docker 镜像 1.5 手动安装 1.6 许可 1.7 准备知识 1.8 联系我们", " scRNA-seq数据分析 碱基吃柠檬 2019-06-26 1 说在前面的话 本项目是将github上经典的scRNA-seq分析教程进行汉化，源项目参见hemberg-lab/scRNA.seq.course 1.1 关于本项目 随着技术发展的日新月异，现在对单个细胞进行高通量测序(scRNA-seq)获得其全基因组的表达谱数据成为可能。scRNA-seq的主要优势是其单细胞水平的分辨率和基因组范围检测能力可以解决其他方法难以解决的问题，比如bulk RNA-seq和RT-qPCR。然而，为了分析scRNA-seq数据，需要新的方法，并且一些针对bulk RNA-seq实验开发的方法的潜在假设也不再适用。 在本课程中，我们将讨论使用scRNA-seq可以解决的一些问题以及可用的计算和统计方法。 本课程通过University of Cambridge的Bioinformatics training unit进行教学，但该页面的材料适用于任何有兴趣了解scRNA-seq数据计算分析的人。 本课程每年讲授两次，每次授课前材料都会更新。 计算工具的数量正在迅速增加，我们正在尽最大努力跟上最新进展。 本课程的主要限制之一是我们倾向使用在R中实现且运行速度相当快的工具。 此外，我们还承认某种程度上偏向于我们或我们的朋友和同事开发的方法。 1.2 视频 该视频于2017年11月录制，当时课程包含的章节少于当前版本。 最新课程的直播录制版见YouTube 1.3 GitHub https://github.com/hemberg-lab/scRNA.seq.course https://github.com/lemonbases/scRNA.seq.course.CN 1.4 Docker 镜像 本课程提供包含所有必须软件的Docker镜像，可通过运行Docker镜像重复本课程的结果。 1.4.1 运行镜像 首先确保您的系统上安装了Docker。如果没有，请参照以下说明进行安装。运行本课程的Docker镜像(使用最新版而不是v3.13)。 docker run -p 8888:8888 -e PASSWORD=&quot;jupyter&quot; quay.io/hemberg-group/scrna-seq-course:v3.13 然后按照以下提供的说明操作, 比如: 要访问NoteBook，在浏览器中打开此文件: file:///home/jovyan/.local/share/jupyter/runtime/nbserver-6-open.html 或复制粘贴以下网址: http://(a9ee1aad5398 or 127.0.0.1):8888/?token=22debff49d9aae3c50e6b0b5241b47eefb1b8f883fcb7e6d Jupyter session会在web浏览器中打开(我们推荐使用Chrome浏览器) 1.4.1.1 Windows用户 Windows操作系统中容器的IP地址与127.0.0.1(localhost)不同。查找IP地址请运行: docker-machine ip default 1.4.2 下载数据或其它文件 请点击New -&gt; Terminal打开一个终端，在新的终端窗口运行: ./poststart.sh 如果您想在Docker镜像外下载数据，您依然可以使用相同poststart.sh脚本，但是您需要在您的计算机上安装AWS CLI。 Alternatively, you can browse and download the files in you web-browser by visiting this link 1.4.3 RStudio 返回到Jupyter浏览界面，将url中的tree更改为rstudio。RStudio server会打开课程文件，软件和数据文件夹。 1.5 手动安装 如果您没有使用本课程的Docker镜像，那么为了能够运行本课程所有的代码块，您需要克隆或下载GitHub仓，并在course_files文件夹中启动R session，然后您还需要手动安装所有必须的软件。 或者您可以只安装感兴趣章节列出的软件。 1.6 许可 本课程所有资料均遵循GPL-3协议。 欢迎任何人浏览材料来学习scRNA-seq数据的分析。 如果您打算将这些材料用于您自己的教学，除了提供合适的引用外，如果您告诉我们，我们将不胜感激。 1.7 准备知识 本课程面向那些基本熟悉Unix和R脚本语言的人。同时我们还假设您熟悉传统bulk RNA-seq数据的比对和分析，以及常用的计算工具。 我们推荐在参加本课程前参加Introduction to RNA-seq and ChIP-seq data analysis 或者 Analysis of high-throughput sequencing data with Bioconductor 1.8 联系我们 如果您有任何的关于课程的意见，问题和建议，请联系Vladimir Kiselev. 译者注 : 或者在该repo下开个issue进行提问。 "],
["introduction-to-single-cell-rna-seq.html", "2 scRNA-seq介绍 2.1 Bulk RNA-seq 2.2 scRNA-seq 2.3 工作流程 2.4 计算分析 2.5 挑战 2.6 实验方法 2.7 如何选择合适的平台", " 2 scRNA-seq介绍 2.1 Bulk RNA-seq 2000后期重大突破，取代微阵列芯片被广泛采用 测量大量混合细胞的平均表达水平 用于比较转录组学，例如比较不同物种同一组织的样本 用于量化整体表达特征，例如疾病研究 不足研究异质性系统，例如早起发育的研究，复杂组织(脑)的研究 不能呈现基因表达随机性 2.2 scRNA-seq 一项新兴技术，第一篇文章为汤富酬发表在Nature method (Tang et al. 2009) 直到~2014年，新测序手段和更低的成本使其流行起来 在大量细胞检测每个基因的表达水平 可以研究新的生物学问题，其中转录组的细胞特异性变化是重要的，比如细胞类型鉴定，细胞响应的异质性，基因表达的随机性，细胞间基因调控网络的推断 研究细胞数目从\\(10^2\\)到\\(10^6\\)个细胞，而且每年递增 目前有不同单细胞的protocol，比如SMART-seq2 (Picelli et al. 2013), CELL-seq (Hashimshony et al. 2012) 和 Drop-seq (Macosko et al. 2015) 也有商业平台, 包括 Fluidigm C1, Wafergen ICELL8 和 10X Genomics Chromium 一些计算分析bulk RNA-seq的方法也适用于 scRNA-seq 大多数情况下 scRNA-seq计算分析需要调整已有方法或者开发新方法 2.3 工作流程 Figure 2.1: 单细胞测序流程 (源自wiki) 总体而言，scRNA-seq实验protocol和bulk RNA-seq类似，我们将在下一章讨论一些最常用的方法。 2.4 计算分析 本课程是用scRNA-seq实验获得的数据进行计算分析。 第一步（黄色）对任何高通量测序数据都是通用的； 后续步骤（橙色）需要整合现有的RNA-Seq分析方法和新方法来解决scRNA-Seq的技术差异； 最后步骤（蓝色），用专门为scRNA-Seq开发的方法进行生物学解释。 Figure 2.2: scRNA-seq分析流程图 目前有几篇关于scRNA-seq分析的综述文章，包括 (Stegle, Teichmann, and Marioni 2015) 目前，也有不同的平台执行上述流程图的一步或多个步骤，包括: Falco 云端单细胞RNA-seq处理框架 SCONE (Single-Cell Overview of Normalized Expression), 单细胞RNA-seq数据质量控制和标准化的包. Seurat 用于单细胞RNA-seq数据质量控制，分析和数据探索的R包. ASAP (Automated Single-cell Analysis Pipeline) 是一个单细胞分析交互式webserver. Bioconductor 是一个分析高通量基因组数据开源，开放式软件项目，包括分析单细胞数据的工具包。 2.5 挑战 scRNA-seq和bulk RNA-seq最大的不同在于每个测序文库是单个细胞，而不是一群细胞。因此需要各位注意来自不同细胞（测序文库）的结果的比较。文库之间差异的主要来源是: 扩增偏好性和扩增效率 （高达1,000,000倍） 基因的’dropout’，其中一个基因在一个细胞中等表达水平，但在另外一个细胞没有检测到(Kharchenko, Silberstein, and Scadden 2014). 在两种情况下， 由于RNA仅来自一个细胞，因此低起始量的转录本是导致差异的主要原因。提高转录本捕获效率和减少扩增偏差是目前活跃的研究灵越。从后续的课程我们也可以看到，可以通过适当的标准化和校正方法来缓解这些问题。 2.6 实验方法 Figure 2.3: 单细胞转录组学摩尔定律 (图片来自 Svensson et al) 开发新的scRNA-seq方法和protocol是目前非常活跃的一个研究领域，在过去的几年中已经发表了一些protocol，不完全列表如下: CEL-seq (Hashimshony et al. 2012) CEL-seq2 (Hashimshony et al. 2016) Drop-seq (Macosko et al. 2015) InDrop-seq (Klein et al. 2015) MARS-seq (Jaitin et al. 2014) SCRB-seq (Soumillon et al. 2014) Seq-well (Gierahn et al. 2017) Smart-seq (Picelli et al. 2014) Smart-seq2 (Picelli et al. 2014) SMARTer STRT-seq (Islam et al. 2013) 这些方法可以按照不同方法进行归类，最重要的两个方面是定量和捕获 定量有两种类型，全长和基于标签。前者试图达到每个转录本均匀覆盖率；然而基于tag的protocol只捕获每个RNA的5’或3’端。量化方法的选择对于数据可用于何种类型的分析具有重要意义。理论上讲，基于全长的protocol可以对整个转录本进行均匀测序，然而通常测序覆盖有偏差。基于tag的protocol主要优势是其可以和唯一的分子标识符结合使用，提高定量准确性(see chapter ??)。另一方面，测序限制在转录组的一端，降低比对map率，并难以区分不同的异构体(Archer et al. 2016)。 捕获的策略决定了通量，细胞如何被选择以及除测序外还可以获得哪种附加信息。三种常用的方法是基于微孔(microwell)-，微流体(microfluidic)-，液滴(droplet)-的方法。 Figure 2.4: 微孔板 (图片来自wiki) 对于基于微孔的平台，使用例如移液管或激光捕获分离细胞并置于微流体孔中。 基于微孔的方法的一个优点是它们可以与荧光激活细胞分选（FACS）结合，基于表面标记物分选细胞。 因此，当想要分离特定细胞群体用于测序时，该策略非常有用。 另一个优点是可以拍摄细胞的照片。 该图像提供了额外的细胞形态，适用于识别包含受损细胞或双份细胞的微孔。 这些方法的主要缺点是它们通常是通量低并且每个单细胞所需的工作量可能相当大。 Figure 2.5: 96孔Fluidigm C1芯片 (图片来自Fluidigm) 微流体平台, 比如 Fluidigm’s C1, 提供了更加集成的系统，用于捕获细胞和进行文库制备所需的过程。 因此，它们提供比基于微孔的平台更高的通量。 通常，在微流体平台中仅捕获约10％的细胞，因此不适合处理稀有细胞类型或非常少量的细胞。 此外，芯片相对昂贵，但由于反应可以以较小的体积进行，因此可以节省试剂。 Figure 2.6: drop-seq方法的原理图 (图片来自Macosko et al) 基于液滴的方法将每个单独的细胞与包括建库所需酶的珠子(bead)一起封装在纳升液滴内。每个珠子都包含唯一的条形码(barcode)，加到所有来自该细胞的序列上。因此可以合并所有液滴进行测序，再基于barcode将序列分配给不同的细胞。Droplet平台通常具有最高的通量，因为建库准备成本约为\\(.05\\) 美元/细胞。相反，测序成本成为其限制因素，通常实验覆盖率低，仅检测到几千个转录本 (Ziegenhain et al. 2017)。 2.7 如何选择合适的平台 最合适的平台取决于手头的生物学问题。 例如，如果研究组织的组成，那么将允许捕获非常大量细胞的基于液滴的方法可能是最合适的。 另一方面，如果人们对具有已知表面标记物的稀有细胞类群感兴趣，那么最好使用FACS进行富集，然后对较少数量的细胞进行测序。 显然，如果研究不同的异构体，那么全长转录物定量将更合适。相比之下，UMI只能与基于tag的protocol一起使用，促进基因水平的量化。 来自Enard group (Ziegenhain et al. 2017)和Teichmann group (Svensson et al. 2017)最近两项研究比较了几种不同的protocol。Ziegenhain等人在同一小鼠胚胎干细胞样本（mESCs）上比较了五种不同的protocol。通过控制细胞数量和测序深度，作者能够直接比较不同protocol的灵敏度，噪声水平和成本。 他们的结论的一个例子如下图所示，不同protocol检测的基因数量（对于给定的检测阈值）差别很大。drop-seq和Smart-seq2之间几乎有两倍的差异，这表明protocol的选择会对研究有重大影响。 Figure 2.7: Enard group 研究 Svensson等采用了另外一种方法，通过使用已知浓度的合成转录本（spike-ins，后面详细介绍）来测量不同protocol的准确性和灵敏度。通过广泛的比较研究，他们同样发现不同protocol间区别较大。 Figure 2.8: Teichmann group 研究 随着实验技术的发展和用于量化技术噪声的计算方法的改进，后续研究有助于我们进一步了解不同方法的优缺点。因为基准测试可以确定哪些策略是最有用的，这些比较研究不仅有助于研究人员决定使用哪种protocol，而且有助于开发新方法。 References "],
["processing-raw-scrna-seq-data.html", "3 scRNA-seq原始数据处理 3.1 FastQC 3.2 移除接头和低质量碱基 3.3 文件格式 3.4 测序文库拆分 3.5 使用STAR比对read 3.6 Kallisto和Pseudo-Alignment", " 3 scRNA-seq原始数据处理 3.1 FastQC 获得单细胞RNA-seq数据后，首先要做的就是检查测序质量，我们使用FastQC来完成此任务。 FastQC是用于测序数据的质量控制工具，既可以用于bulk RNA-seq，也可以用于scRNA-seq。 FastQC将测序数据作为输入，并返回测序质量的报告。访问以下链接获取更多关于FastQC的信息： https://www.bioinformatics.babraham.ac.uk/projects/fastqc/ 该网站包含下载和安装FastQC的链接以及生成报告的文档。幸运的是，我们已经为您安装了FastQC1，因此这里我们将查看文档。将网页向下滚动到“示例报告”，然后单击“Good Illumina Data”。 这里给出了高质量Illumina测序数据立项报告的示例。 现在让我们自己生成一份FastQC报告。 今天，我们将使用(Kolodziejczyk et al. 2015)生成的mESC单细胞数据集进行分析。细胞使用SMART-seq2 protocol构建测序文库并进行双端测序。这些文件位于Share文件夹中。 注意 本课程的当前文本是为参加我们课程的人员编写的。您必须下载文件（ERR522959_1.fastq和ERR522959_2.fastq）并创建Share目录才能运行命令。你可以在这里找到这些文件： https://www.ebi.ac.uk/arrayexpress/experiments/E-MTAB-2600/samples/ 现在让我们来看看文件: less Share/ERR522959_1.fastq less Share/ERR522959_2.fastq 任务1: 尝试找出生成FastQC报告的命令，提示: 尝试执行 fastqc -h 该命令将告诉您可以传递给FastQC的参数。如果您遇到困难，请随时寻求帮助！ 如果运行成功，则应为正向和反向reads 都会生成.zip和.html文件。运行成功后，请跳到下一节。 3.1.1 解决方案并下载报告 如果还没有成功，请使用以下命令生成FastQC报告： mkdir fastqc_results fastqc -o fastqc_results Share/ERR522959_1.fastq Share/ERR522959_2.fastq 一旦命令执行完毕，您应该总共有四个文件 - 每个read对应一个zip和html文件，报告位于html文件中。 要查看它，我们需要使用filezilla或scp将它从AWS下载到您的计算机上。 下载到本地后，单击打开您的FastQC报告。记得要查看正向和反向read的质量报告！测序的质量如何？有什么我们应该关注的问题吗？ 我们如何解决这些问题呢？ 3.2 移除接头和低质量碱基 Trim Galore是一个cutadapt的封装，用于移除测序接头序列和测序末端的低质量碱基。 鉴于FastQC报告中存在一些接头污染，最好从数据中移除接头序列。 任务2：数据中使用了哪种类型的接头序列？提示：查看FastQC报告“Adapter Contern”图。 现在让我们使用Trim Galore移除那些有问题的接头序列，修剪后再次检查读取质量，使用FastQC生成另一个报告。 任务3：找出移除adapter的命令。提示1：您可以使用 trim_galore -h 查看Trim Galore的参数描述。 提示2：仔细阅读上述命令的输出。本实验中使用的接头序列非常常见。您是否需要知道接头的实际序列才能将其删除？ 任务4: 为清洗后的数据生成FastQC报告。接头序列污染消失了吗？ 一旦您认为您已成功去除接头序列并通过FastQC确认，请使用下一部分核验您的结果。 3.2.1 Solution 您可以使用以下命令去除Nextera测序接头序列： mkdir fastqc_trimmed_results trim_galore --nextera -o fastqc_trimmed_results Share/ERR522959_1.fastq Share/ERR522959_2.fastq 请记住为清洗后的数据文件重新生成FastQC报告！FastQC现在应该显示’Adaptor Content’为’pass’了。 祝贺您现在已生成测序质量报告并移除接头序列。在下一步中，我们将使用STAR和Kallisto将清洗后的reads比对到参考转录组上。 3.3 文件格式 3.3.1 FastQ FastQ是scRNA-seq数据中最原始数据格式。所有scRNA-seq protocol都使用双端测序，根据使用的protocol，条形码序列(barcode)可能出现在paired-reads中一条或两条上。但是使用唯一分子标识符(UMIs) 的protocol会生成包含细胞和UMI条形码加上接头序列但是没有转录本序列的read。因此虽然是双端测序，但比对时按照单端测序对待。 FastQ文件格式如下: &gt;ReadID READ SEQUENCE + SEQUENCING QUALITY SCORES 3.3.2 BAM BAM文件以标准且高效的方式存储比对结果。SAM文件为直接可读的，而BAM文件是高度压缩的版本。BAM / SAM文件包含头部信息，通常包括样本制备，测序和比对的信息;后面为每个read的比对结果，以tab作为分隔符。 比对行标准格式如下: QNAME : read名称(如果为UMI文库，则包括UMI条形码) FLAG : 数字指示reads比对的类型, link该网站有所有可能的类型 RNAME : 参考序列名称 (比如比对到的染色体名称). POS : 最左边比对位置 MAPQ : 比对质量 CIGAR : 表示reads中匹配/不匹配部分 (可能包括soft-clipping). RNEXT : mate/next reads比对到的参考序列名称 PNEXT : mate/next reads比对到的第一个碱基位置 TLEN : 模板长度（read比对到的参考区域的长度） SEQ : read序列 QUAL : read质量 BAM/SAM 文件可通过’samtools’互相转换: samtools view -S -b file.sam &gt; file.bam samtools view -h file.bam &gt; file.sam 一些测序设备会自动将测序reads比对到标准基因组上，并提供BAM或CRAM格式文件。通常基因组中不包含ERCC序列，因此不会又ERCCs reads比对到在BAM / CRAM文件中。 要量化ERCC（或任何其他遗传变异），或者如果您只想使用不同于标准流程的比对算法（通常过时），那么您将需要将BAM / CRAM文件转换回FastQs: BAM文件可以使用bedtools转为FastQ。为避免比对到多个基因组位置的一个read转换为FastQ多条read，首先将BAM文件按读取名称排序，并使用samtools删除次级比对。Picard也包含将BAM转换为FastQ文件的方法。 # sort reads by name samtools sort -n original.bam -o sorted_by_name.bam # remove secondary alignments samtools view -b -F 256 sorted_by_name.bam -o primary_alignment_only.bam # convert to fastq bedtools bamtofastq -i primary_alignment_only.bam -fq read1.fq -fq2 read2.fq 3.3.3 CRAM CRAM文件类似SAM文件，其头部信息包括比对使用的参考基因组信息，这使得read中和参考基因组一样的碱基可以进一步压缩。与BAM相比，CRAM还支持有损数据压缩方法以进一步优化存储。CRAM主要由Sanger/EBI测序机构使用。 CRAM和BAM文件可以使用最新版本的samtools（&gt; = v1.0）进行格式转换。但是，这种转换可能需要将参考基因组下载到缓存中。 或者，您可以从CRAM文件的头部元数据预先下载参考基因组，或者询问生成CRAM文件的人获得参考基因组，并使用’-T’指定该文件。因此我们建议在执行此操作之前设置特定的缓存位置： export REF_CACHE=/path_to/cache_directory_for_reference_genome samtools view -b -h -T reference_genome.fasta file.cram -o file.bam samtools view -C -h -T reference_genome.fasta file.bam -o file.cram 3.3.4 手动查看文件 有时，手动检查文件可能很有用，例如检查文件的头部信息。’less’和’more’可在命令行查看任何文本文件。管道符|可以在多个命令之间传输数据，省却把中间数据存储多个拷贝的过程。 less file.txt more file.txt # counts the number of lines in file.txt wc -l file.txt samtools view -h file.[cram/bam] | more # counts the number of lines in the samtools output samtools view -h file.[cram/bam] | wc -l 练习 现提供cram示例文件: EXAMPLE.cram 任务1: 这个文件是怎么生成的？使用了什么软件？参考基因组时什么？(提示: 检查头部信息) 任务2: 有多少reads比对上/没有比对上？总共有多少reads？secondary alignments有多少? (提示: 使用FLAG) 任务3: 将CRAM转为Fastq文件。转换后的read只有一个拷贝吗？(将转换后的Fastq文件命名为“10cells_read1.fastq” “10cells_read2.fastq”) 如果您遇到问题，可以通过输入命令来显示每个软件的帮助信息 - 例如 ‘samtools view’，‘bedtools’ 答案 samtools view -T data/2000_reference.transcripts.fa -H data/EXAMPLE.cram | more samtools view -T data/2000_reference.transcripts.fa -f 4 data/EXAMPLE.cram | wc -l # unmapped samtools view -T data/2000_reference.transcripts.fa -F 260 data/EXAMPLE.cram | wc -l # mapped samtools view -T data/2000_reference.transcripts.fa -F 256 data/EXAMPLE.cram | wc -l # total samtools view -T data/2000_reference.transcripts.fa -f 256 data/EXAMPLE.cram | wc -l # secondary alignments samtools view -b -h -T data/2000_reference.transcripts.fa data/EXAMPLE.cram -o data/EXAMPLE.bam samtools sort -n data/EXAMPLE.bam -o data/sorted_EXAMPLE.bam samtools view -b -F 256 data/sorted_EXAMPLE.bam -o data/primary_EXAMPLE.bam # convert to fastq bedtools bamtofastq -i data/primary_EXAMPLE.bam -fq data/10cells_read1.fq -fq2 data/10cells_read2.fq 3.3.5 Genome (FASTA, GTF) 为了比对序列，需要参考基因组和基因组注释文件(GTF或者GFF格式)。模式生物的基因组和注释文件可以从目前主流的基因组数据库下载: Ensembl, NCBI, 或者 UCSC Genome Browser. GTF文件包括基因，转录本和外显子的注释，格式如下： (1) seqname : 染色体/scaffold编号 (2) source : 注释来源 (3) feature : 注释信息类型(比如基因，转录本，外显子) (4) start : 起始位置 (bp) (5) end : 终止 (bp) (6) score : 得分 (7) strand : + (正链) or - (负链) (8) frame : 仅对CDS有效，起始编码位置，或者到达下一个密码子需要跳过的碱基个数 (0 = first base, 1 = second base, etc..) (9) attribute : ;分割的键值对来显示其它信息 (比如 names/IDs, biotype) 空字段用“.”填充 根据我们的经验，Ensembl是最容易使用的，并且具有最大的注释集。NCBI往往更严格，仅包括置信度高的基因注释。 而UCSC包含多个使用不同标准的基因组注释。 如果您的实验系统包含非标准序列，则必须将这些序列添加到基因组fasta和gtf中来定量它们的表达。 最常见的是针对ERCC spike-ins，CRISPR相关的序列或其他过表达/报告载体。 为了获得最大的可用性/灵活性，我们建议为添加的任何非标准序列创建完整和详细的fasta序列和gtf序列。 目前没有标准化的方法来做到这一点 以下是我们的自定义perl脚本，用于为ERCC创建一个gtf和fasta文件，可以将其附加到基因组中。如果要量化内含子读数时，您可能还需要更改gtf文件以处理内含子中的重复元素。任何脚本语言甚至“awk”或一些文本编辑器都可以用来相对有效地完成这项任务，但它们超出了本课程的范围。 # Converts the Annotation file from # https://www.thermofisher.com/order/catalog/product/4456740 to # gtf and fasta files that can be added to existing genome fasta &amp; gtf files. my @FASTAlines = (); my @GTFlines = (); open (my $ifh, &quot;ERCC_Controls_Annotation.txt&quot;) or die $!; &lt;$ifh&gt;; #header while (&lt;$ifh&gt;) { # Do all the important stuff chomp; my @record = split(/\\t/); my $sequence = $record[4]; $sequence =~ s/\\s+//g; # get rid of any preceeding/tailing white space $sequence = $sequence.&quot;NNNN&quot;; my $name = $record[0]; my $genbank = $record[1]; push(@FASTAlines, &quot;&gt;$name\\n$sequence\\n&quot;); # is GTF 1 indexed or 0 indexed? -&gt; it is 1 indexed # + or - strand? push(@GTFlines, &quot;$name\\tERCC\\tgene\\t1\\t&quot;.(length($sequence)-2).&quot;\\t.\\t+\\t.\\tgene_id \\&quot;$name-$genbank\\&quot;; transcript_id \\&quot;$name-$genbank\\&quot;; exon_number \\&quot;1\\&quot;; gene_name \\&quot;ERCC $name-$genbank\\&quot;\\n&quot;); push(@GTFlines, &quot;$name\\tERCC\\ttranscript\\t1\\t&quot;.(length($sequence)-2).&quot;\\t.\\t+\\t.\\tgene_id \\&quot;$name-$genbank\\&quot;; transcript_id \\&quot;$name-$genbank\\&quot;; exon_number \\&quot;1\\&quot;; gene_name \\&quot;ERCC $name-$genbank\\&quot;\\n&quot;); push(@GTFlines, &quot;$name\\tERCC\\texon\\t1\\t&quot;.(length($sequence)-2).&quot;\\t.\\t+\\t.\\tgene_id \\&quot;$name-$genbank\\&quot;; transcript_id \\&quot;$name-$genbank\\&quot;; exon_number \\&quot;1\\&quot;; gene_name \\&quot;ERCC $name-$genbank\\&quot;\\n&quot;); } close($ifh); # Write output open(my $ofh, &quot;&gt;&quot;, &quot;ERCC_Controls.fa&quot;) or die $!; foreach my $line (@FASTAlines) { print $ofh $line; } close ($ofh); open($ofh, &quot;&gt;&quot;, &quot;ERCC_Controls.gtf&quot;) or die $!; foreach my $line (@GTFlines) { print $ofh $line; } close ($ofh); 3.4 测序文库拆分 文库拆分根据Protocol不同或构建的流程不同需要有对应的处理方式。我们所知道的最灵活的文库拆分工具是zUMIs，可用于拆分和比对大多数基于UMI的protocol。对于Smartseq2或其他全长转录本双端测序protocol，数据通常已经被拆分好。诸如GEO或ArrayExpress之类的公共存储库需要在上传之前对基于小规模/基于板的scRNASeq数据拆分，并且许多测序公司在数据返回给您之前自动拆分数据。如果您没有使用已发表的流程，并且数据未被测序公司拆分，则您必须自己对其进行文库拆分。因为不同的建库方案引入的barcode序列的长度和位置不同，通常都需要自己写脚本解决。 对于所有数据类型，文库拆分涉及从一端或双端短序列中识别并移除细胞条形码序列(cell-barcode)。如果提前知道加入的cell-barcodes，比如数据来自基于PCR板的protocol，需要将每个cell-barcode与预期的cell-barcode进行比对，并将其归类于最相近的cell-barcode(根据cell-barcode的设计，一般允许最多1-2错配)。这些数据通常在比对之前进行拆分，从而可以并行比对。 我们提供公开可用perl脚本，可以拆分任何有或没有UMI 的plate-based的建库方案生成的数据，用法如下： perl utils/1_Flexible_UMI_Demultiplexing.pl data/10cells_read1.fq data/10cells_read2.fq &quot;C12U8&quot; data/10cells_barcodes.txt 2 Ex Barcode Structure: 12 bp CellID followed by 8 bp UMI Doesn&#39;t match any cell: 0 Ambiguous: 0 Exact Matches: 400 Contain mismatches: 0 Input Reads: 400 Output Reads: 400 perl utils/1_Flexible_FullTranscript_Demultiplexing.pl data/10cells_read1.fq data/10cells_read2.fq &quot;start&quot; 12 data/10cells_barcodes.txt 2 Ex Doesn&#39;t match any cell: 0 Ambiguous: 0 Exact Matches: 400 Contain Mismatches: 0 Input Reads: 400 Output Reads: 400 对于包含UMI的数据，文库拆分包括将UMI code附加到包含基因区的序列read名字上。如果数据来自droplet-based protocol或SeqWell，其中预期条形码的数量远远高于预期的细胞数量，为避免生成才能大量的文件么cell-barcode也加到测序read的名字后面。在这些情况下，在量化步骤期间进行文库拆分，以便于识别来源于完整细胞而不是背景噪声的cell-barcode。 3.4.1 鉴定含有细胞的液滴/微孔 基于液滴的scRNA-seq方法，只有一部分液滴包含bead和一个完整的细胞。然而生物实验可能不理想，一些RNA会从死细胞/受伤细胞中泄露出去。因此，没有完整细胞的液滴可能捕获少量环境游离RNA，这些RNA将进入测序文库，出现在最终测序结果中。液滴大小，扩增效率和测序的变化将导致“背景”和真实细胞文库大小区别很大。目前已有很多方法用来区分对应真实细胞的cell-barcode 已经使用各种方法来试图区分对应于真实细胞的那些细胞条形码。 大多数方法使用每个barcode的总分子数（可以应用于总reads）并尝试寻找“break point”，区分来自真实细胞较大的文库和来自背景较小的文库。下面加载包含大小文库细胞的示例模拟数据： umi_per_barcode &lt;- read.table(&quot;data/droplet_id_example_per_barcode.txt.gz&quot;) truth &lt;- read.delim(&quot;data/droplet_id_example_truth.gz&quot;, sep=&quot;,&quot;) 练习 多少唯一的barcode被检测到？ 数据中多少来自真实的细胞？ 为简化计算，去除所有少于10个分子的barcode。 答案 dim(umi_per_barcode)[1] dim(truth)[1] 一种方法是寻找每个条形码对应总分子突然下降的拐点。 One approach is to look for the inflection point where the total molecules per barcode suddenly drops: barcode_rank &lt;- rank(-umi_per_barcode[,2]) plot(barcode_rank, umi_per_barcode[,2], xlim=c(1,8000)) 可以看出文库大小近似指数分布，简单起见，对数据进行log转换。 log_lib_size &lt;- log10(umi_per_barcode[,2]) plot(barcode_rank, log_lib_size, xlim=c(1,8000)) 从上图可以看出，拐点更加明显了。我们可以手动估计拐点的位置，但是用算法估计更加精确，以及可重复。 # inflection point o &lt;- order(barcode_rank) log_lib_size &lt;- log_lib_size[o] barcode_rank &lt;- barcode_rank[o] rawdiff &lt;- diff(log_lib_size)/diff(barcode_rank) inflection &lt;- which(rawdiff == min(rawdiff[100:length(rawdiff)], na.rm=TRUE)) plot(barcode_rank, log_lib_size, xlim=c(1,8000)) abline(v=inflection, col=&quot;red&quot;, lwd=2) threshold &lt;- 10^log_lib_size[inflection] cells &lt;- umi_per_barcode[umi_per_barcode[,2] &gt; threshold,1] TPR &lt;- sum(cells %in% truth[,1])/length(cells) Recall &lt;- sum(cells %in% truth[,1])/length(truth[,1]) c(TPR, Recall) ## [1] 1.0000000 0.7831707 另外一种方法是构建混合模型，找出重叠区域的最高和最低分布。然而数据可能并不满足假定的分布。 set.seed(-92497) # mixture model require(&quot;mixtools&quot;) ## 载入需要的程辑包：mixtools ## mixtools package, version 1.1.0, Released 2017-03-10 ## This package is based upon work supported by the National Science Foundation under Grant No. SES-0518772. mix &lt;- normalmixEM(log_lib_size) ## number of iterations= 43 plot(mix, which=2, xlab2=&quot;log(mol per cell)&quot;) p1 &lt;- dnorm(log_lib_size, mean=mix$mu[1], sd=mix$sigma[1]) p2 &lt;- dnorm(log_lib_size, mean=mix$mu[2], sd=mix$sigma[2]) if (mix$mu[1] &lt; mix$mu[2]) { split &lt;- min(log_lib_size[p2 &gt; p1]) } else { split &lt;- min(log_lib_size[p1 &gt; p2]) } 练习 使用分割点识别细胞并计算TPR和Recall 答案 cells &lt;- umi_per_barcode[umi_per_barcode[,2] &gt; 10^split,1] TPR &lt;- sum(cells %in% truth[,1])/length(cells) Recall &lt;- sum(cells %in% truth[,1])/length(truth[,1]) c(TPR, Recall) 第三种方法，CellRanger假设真实细胞文库大小变化范围在10倍以内，然后用哪个期望的细胞数目估计区间的分布。 n_cells &lt;- length(truth[,1]) # CellRanger totals &lt;- umi_per_barcode[,2] totals &lt;- sort(totals, decreasing = TRUE) # 99th percentile of top n_cells divided by 10 thresh = totals[round(0.01*n_cells)]/10 plot(totals, xlim=c(1,8000)) abline(h=thresh, col=&quot;red&quot;, lwd=2) 练习 用该阈值识别细胞并计算TPR和Recall。 答案 cells &lt;- umi_per_barcode[umi_per_barcode[,2] &gt; thresh,1] TPR &lt;- sum(cells %in% truth[,1])/length(cells) Recall &lt;- sum(cells %in% truth[,1])/length(truth[,1]) c(TPR, Recall) 最后EmptyDrops，目前处于Beta测试阶段，其实用所有液滴的基因 × 细胞分子计数矩阵，从具有极低counts的液滴估计背景RNA的分布，然后寻找与背景基因表达模式不同的细胞。背景RNA通常和群体中大部分细胞的表达模式相似，该方法与拐点方法相结合。因此，EmptyDrops是唯一能够识别高度多样化样本中鉴定小群体细胞条形码的方法。 下面提供了该方法的运行代码：（我们会根据官方发布及时更新代码） require(&quot;Matrix&quot;) raw.counts &lt;- readRDS(&quot;data/droplet_id_example.rds&quot;) require(&quot;DropletUtils&quot;) # emptyDrops set.seed(100) e.out &lt;- emptyDrops(my.counts) is.cell &lt;- e.out$FDR &lt;= 0.01 sum(is.cell, na.rm=TRUE) plot(e.out$Total, -e.out$LogProb, col=ifelse(is.cell, &quot;red&quot;, &quot;black&quot;), xlab=&quot;Total UMI count&quot;, ylab=&quot;-Log Probability&quot;) cells &lt;- colnames(raw.counts)[is.cell] TPR &lt;- sum(cells %in% truth[,1])/length(cells) Recall &lt;- sum(cells %in% truth[,1])/length(truth[,1]) c(TPR, Recall) 3.5 使用STAR比对read 现在我们已经对测序数据进行质控，并且获得了高质量的清洗后数据，下一步将其比对到参考基因组上。如果我们想要定量基因表达或筛选样品间差异表达的基因，则通常需要某种形式的比对。 短序列比对的工具有很多，目前我们主要关注两个。第一个工具是STAR (Dobin et al. 2013)，对于测序数据中的每一个read，STAR尝试寻找参考基因组中一个或多个最长可能序列。例如，下图所示，read(蓝色)跨越两个外显子和一个可变剪切位点(紫色)。STAR能够找出read的第一部分和序列的第一个外显子仙童，同时第二部分和序列的第二个外显子相同。因为STAR可能够以这种方式识别剪切事件，又被称为“spice aware alinger”。 Figure 3.1: STAR比对示意图, 来自Dobin et al. 通常，STAR将reads比对到参考基因组是允许其检测新的剪接事件或染色体重排。然而，STAR的一个问题是它需要大量的内存，特别是当参考基因组很大（例如，小鼠和人类）。为了加速我们今天的分析，我们将使用STAR将reads与只包含2000个转录本的参考转录组进行比对。请注意，这不是正常或推荐的做法，我们只是出于时间原因这样做。我们建议比对到参考基因组。 STAR对齐需要两个步骤。第一步，用户向STAR提供参考基因组序列（FASTA）和注释文件（GTF），STAR用它来创建基因组索引。 第二步，STAR将用户的reads比对到基因组索引。 首先现在创建索引。请记住，由于时间的原因，我们比对到转录组而不是基因组，这意味着我们只需要向STAR提供比对的转录本序列。可以从Ensembl获取许多模式生物的转录组数据。 任务 1: 执行以下命令创建索引: mkdir indices mkdir indices/STAR STAR --runThreadN 4 --runMode genomeGenerate --genomeDir indices/STAR --genomeFastaFiles Share/2000_reference.transcripts.fa 任务 2: STAR用到的每个参数意义? 提示: 参考STAR帮助文档 任务 3: 如果比对到基因组而不是转录组，命令是什么？ 现在索引创建完成，进行比对步骤。 任务 4: 尝试找出应该使用什么命令将我们的清洗后的数据（从ERR522959）比对索引上。参考使用STAR手册， 您认为知道答案，检查它是否与下一节中的解决方案匹配并执行比对。 任务 5: 尝试了解比对结果文件。 3.5.1 STAR比对命令 使用以下命令完成比对: mkdir results mkdir results/STAR STAR --runThreadN 4 --genomeDir indices/STAR --readFilesIn Share/ERR522959_1.fastq Share/ERR522959_2.fastq --outFileNamePrefix results/STAR/ 3.6 Kallisto和Pseudo-Alignment STAR是read比对工具，而Kallisto是伪比对工具(Bray et al. 2016)。它们的主要区别是aligner比对到参考基因组或转录组上，而pseudo-aligner将k-mers比对到参考基因组或转录组上。 3.6.1 k-mer是什么? k-mer是来自reads长度为k的序列。例如，假设read为ATCCCGGGTTAT，从中制作7-mer。为此，提取read的前七个碱基找到第一个7-mer。然后向下移动一个碱基得到第二个7-mer，然后计算接下来的七个碱基。图2显示了可以从read得到的所有7-mers： Figure 3.2: 示例read产生7-mer 3.6.2 为什么比对k-mer而不是reads? 主要有两个原因： psudi-aligners使用计算技巧使得比对k-mers比传统比对reads要快，具体细节参考(Bray et al., 2017) 在一些情况下，pseudo-aligners比对传统的比对工具更好地处理测序错误。比如，假设序列的第一个碱基存在测序错误，实际为T但是测序为A，这会影响pseduo-aligners比对第一个7-mer，而不影响后面7-mers的比对。 3.6.3 Kallisto伪比对模式 Kallisto有一个为scRNA-seq实验特别设计的伪比对模式。与STAR不同的是，Kallisto比对到参考转录组而不是参考基因组。 这意味着Kallisto将reads比对到剪接异构体而不是基因，然而对scRNA-seq而言，这很有挑战性： 由于以下原因，对单细胞RNA-seq特异性地将读数映射到同种型而不是基因： scRNA-seq覆盖率比bulk RNA-seq低，意味着从reads获得的信息减少。 许多scRNA-seq protocol 在3’端覆盖存在偏差，意味着如果两个异构体只在5’端不同，则很难确定read来自哪个异构体。 一些scRNA-seq protocol测序读段较短，难以区分read来自哪个异构体。 Kallisto的pseudo mode和pseudo alignment略有不同。Kallisto不与异构体比对，而是与等价类(equivalence classes)比对。如果read比对到多个异构体上，Kallisto会记录read比对到包含有所有异构体的等价类，因此可以使用等价类计数而非基因或转录本计数用于下游的聚类等分析。具体见下图： Figure 3.3: Kallisto等价类示意图, 来自 https://pachterlab.github.io/kallisto/singlecell.html. 今天我们只用1个细胞进行pseudo-alignment, 但是Kallisto可以同时对大量细胞进行pseudo-alignment以及使用UMI信息，具体参看官方文档 类似STAR，pseudo-alignment之前需要用Kallisto生成索引。 任务 6: 使用以下命令构建Kallisto索引, 使用官方手册理解每个参数的意义 mkdir indices/Kallisto kallisto index -i indices/Kallisto/transcripts.idx Share/2000_reference.transcripts.fa 任务 7: 使用Kallisto手册找出运行pseudo-alignment的命令，如果您认为知道答案，可以去下一节核验并运行pseudo-alignment。 3.6.4 Kallisto Pseudo-Alignment命令 使用以下命令执行pseudo-alignment mkdir results/Kallisto kallisto pseudo -i indices/Kallisto/transcripts.idx -o results/Kallisto -b batch.txt 参考官方手册构建batch.txt。 3.6.5 理解Kallisto输出结果 上述命令会生成4个文件 - matrix.cells, matrix.ec, matrix.tsv and run_info.json. matrix.cells 细胞ID列表. 因为我们只使用一个细胞，该文件应该只包含“ERR522959” matrix.ec 等价类信息。每一行的第一个数字为等价类ID，第二个数字对应等价类的转录本ID。比如，“10 1,2,3”表示等价类10包括3个转录本，分别是1,2,3. ID顺序对应转录本在参考转录本的顺序。采用0-base技术，转录本ID为1,2,3分别对应2000_reference.transcripts.fa中第2,3,4个转录本。 matrix.tsv 每个等价类在不同cell的read count信息。第一个数字为等价类ID，如matrix.ec一样。第二个数字为cell ID，和matrix.cells文件中细胞名字对应。第三个数字等价类在该细胞的reads count。比如，“5 1 3”表示来自细胞1的3个reads比对到等价类5上。这里的细胞ID同样是0-based，所以细胞1对应matrix.cells的第二行。 Note that zero indexing is used, so cell 1 corresponds to the second line of matrix.cells. run_info.json Kallisto执行信息，一般可忽略。 References "],
["construction-of-expression-matrix.html", "4 构建表达矩阵 4.1 质量控制 4.2 Reads 比对 4.3 比对示例 4.4 Mapping QC 4.5 Reads定量 4.6 唯一分子标识符", " 4 构建表达矩阵 很多scRNA-seq数据分析从表达矩阵为开始。 一般来说，表达矩阵的每一行代表一个基因，每列代表一个细胞（但是一些作者会使用转置）。 每个条目代表特定基因在特定细胞的表达水平。表达量的单位取决于protocol和标准化方法。 4.1 质量控制 scRNA-seq实验测序结果是大量的cDNA reads。第一步是确保测序的高质量，可以使用以下标准工具执行质量控制，例如 FastQC 或 Kraken. 假设有experiment.bam文件,运行以下FASTQC命令 $&lt;path_to_fastQC&gt;/fastQC experiment.bam 下面是125 bp reads数据集的FastQC输出结果示例。下图显示了由于技术错误导致在read中心无法正确测序几个碱基。 但是，由于read的其余部分质量很高，因此该错误很可能对比对效率的影响可以忽略不计。 Figure 4.1: Example of FastQC output 另外，使用Integrative Genomics Browser (IGV)或者SeqMonk对数据进行可视化非常有帮助。 4.2 Reads 比对 将reads低质量碱基取出后，把剩下的序列比对到参考基因组上。同样，没有专门为scRNA-seq设计的比对方法。我们可以使用STAR或者TopHat进行比对。对来自有丰富注释信息模式生物(比如小鼠和人)的大型全长转录本数据集，pseudo-alignment方法(比如Kallisto，Salmon)可能比传统比对方法表现更好。基于drop-seq数据集包含数十万reads，pseudoaligners运行时间比传统比对工具快不止一个量级。 使用STAR比对示例 $&lt;path_to_STAR&gt;/STAR --runThreadN 1 --runMode alignReads --readFilesIn reads1.fq.gz reads2.fq.gz --readFilesCommand zcat --genomeDir &lt;path&gt; --parametersFiles FileOfMoreParameters.txt --outFileNamePrefix &lt;outpath&gt;/output 注意, 如果使用了spike-ins, 在比对前应将spike-ins的DNA序列添加到参考基因组序列中. 注意, 当使用UMIs，应该从read序列中取出条形码序列。通常做法是将barcode加到read名称上。 一旦将每个细胞的reads比对到参考基因组，我们需要确保每个细胞有足够数量的read比对到参考基因组。 根据我们的经验，小鼠或人类细胞的reads的map率为60-70％。但是此结果可能会因protocols，read长度和read比对工具参数而异。一般来说，我们希望所有细胞都具有相似的map率，因此应检查并可能删除任何异常值，map率低通常表示污染。 使用Salmon定量基因表达： $&lt;path_to_Salmon&gt;/salmon quant -i salmon_transcript_index -1 reads1.fq.gz -2 reads2.fq.gz -p #threads -l A -g genome.gtf --seqBias --gcBias --posBias 注意：Salmon或得到估计read counts和估计transcripts per million(tpm), TMP对scRNA-seq长基因的表达进行了过度校正，因此我们建议使用read counts。 4.3 比对示例 下列直方图显示scRNA-seq实验每个细胞比对reads总数。每个条形代表一个细胞，按照每个细胞的总reads数升序排列。三个红色箭头表示比对覆盖率降低的异常细胞，应该在后续分析中将其去除。黄色箭头表示unmapped reads较多的细胞。在比对质控步骤我们保留这两个细胞，但是在细胞质控时由于高的核糖体RNA reads比例将其移除。 Figure 4.2: Example of the total number of reads mapped to each cell. 4.4 Mapping QC 将原始测序数据比对到基因组后，需要评估比对的质量。目前有很多方法对比对质量进行评估，包括：rRNA/tRNAs reads数目，uniquely mapping reads比例，跨剪切位点的reads数，转录本read深度。为bulk RNA-seq开发的方法，比如RSeQC也适合单细胞数据： python &lt;RSeQCpath&gt;/geneBody_coverage.py -i input.bam -r genome.bed -o output.txt python &lt;RSeQCpath&gt;/bam_stat.py -i input.bam -r genome.bed -o output.txt python &lt;RSeQCpath&gt;/split_bam.py -i input.bam -r rRNAmask.bed -o output.txt 然而预期结果依赖于实验protocol，比如很多scRNA-seq方法是用poly-A富集来排除rRNA，但导致read覆盖率具有3’偏好性，即基因的3’区域更容易被检测到。下图展示了测序reads的3’偏好性，以及3个从数据集中移除的异常细胞。 Figure 4.3: Example of the 3’ bias in the read coverage. 4.5 Reads定量 下一步是定量每个细胞的基因表达水平。对于mRNA数据，可以使用针对bulk RNA-seq开发的工具，比如HT-seq 或者FeatureCounts # include multimapping &lt;featureCounts_path&gt;/featureCounts -O -M -Q 30 -p -a genome.gtf -o outputfile input.bam # exclude multimapping &lt;featureCounts_path&gt;/featureCounts -Q 30 -p -a genome.gtf -o outputfile input.bam Unique molecular identifiers (UMIs) 使得计算分子的绝对数目成为可能，并且在scRNA-seq非常受欢迎。下一章将讨论如何处理UMIs。 4.6 唯一分子标识符 感谢来 EMBL Monterotondo的Andreas Buness在本节的合作。 4.6.1 UMI介绍 UMI是在反转录过程中添加到转录本中的短（4-10bp）随机条形码序列。它们使测序read能够对应到单个转录物，从而去除scRNA-Seq数据扩增噪声和偏差。 Figure 4.4: UMI sequencing protocol 当对包含UMI数据测序时，仅对包含UMI的转录本末端进行测序（通常是3’末端） 4.6.2 比对条形码序列 由于条形码数量(\\(4^N\\),\\(N\\)为UMI的长度)比细胞中RNA分子(~\\(10^6\\))数目多，每个barcode通常会连接多个转录本。因此需要barcode和转录本比对位置来鉴定转录本分子。第一步比对UMI reads，推荐使用STAR，因为其运行速度快并且输出高质量BAM比对。此外，比对位置对鉴定转录本新的3’UTR很有帮助。 UMI测序通常由双端reads组成，其中一端read捕获细胞和UMI条形码，然后另一端read包含转录本的外显子序列(Figure 4.5)。注意：推荐移除reads中poly-A序列避免比对到基因/转录本内部的poly-A/poly-T序列而产生错误。 处理完UMI实验的reads后，通常有以下惯例： UMI加到另外一个配对read的序列名称中 reads按照cell barcode归类到不同的文件，对特别大，测序深度浅的数据集，cell barcode加到read名称中以减少文件数量。 Figure 4.5: UMI测序reads, 红色闪电代表不同片段的文职 4.6.3 Barcodes计数 理论上，每个唯一的UMI-转录本对应该对应来自一个RNA分子的所有reads，然而实际情况并非如此，常见原因如下： 不同UMI不一定表示为不同的分子，由于PCR或测序错误，碱基对替换事件可产生新的UMI序列。 较长的UMI碱基替换的可能性更高。根据cell barcode测序误差估计，7-10％的10bp UMI至少会包含一个错误。如果没有纠正错误，将导致高估转录本的数量。 不同转录本不一定是不同分子，比对错误，或者multimapping reads可能导致某些UMI对应到错误的基因/转录本，这种类型的错误也会导致高估转录本的数量。 相同的UMI不一定是相同分子，UMI频率和短UMI可导致相同UMI连接到相同基因的不同mRNA分子。因此，将导致低估转录本数量。 Figure 4.6: UMIs中可能错误 4.6.4 错误校正 如何最好地解释UMI中的错误仍然是一个活跃的研究领域。我们认为解决上述问题的最佳方法是： UMI-tools 使用directional-adjacency方法，同时考虑错配数目和相似UMIs相对频率来识别可能的PCR/测序错误。 目前问题还没完全解决，通过删除很少reads支持的UMI-转录本对，或者移除multi-mapping reads可以减轻该问题。 简单饱和校正 (又称 “collision probability”) Grun, Kester and van Oudenaarden (2014) 估计真实的分子数目 \\(M\\): \\[M \\approx -N*log(1 - \\frac{n}{N})\\] 其中N=唯一UMI barcode的总数，n=观测barcode数目 该方法的一个重要缺陷是其假设所有UMI出现频率相同。大多数情况下并不是，因为GC含量不同引入偏差。 Figure 4.7: 基因扩增效率 如何最好地处理和使用UMI目前是生物信息学界的一个活跃的研究领域。最近开发的几种方法，包括： UMI-tools PoissonUMIs zUMIs dropEst 4.6.5 下游分析 目前UMI平台(DropSeq, InDrop, ICell8)捕获效率从低到高差异很大，如下图所示。 Figure 4.8: 捕获效率差异 这种差异引入强烈的偏差，需要在下游分析中考虑。最近的分析通常基于细胞类型或生物通路吧细胞/基因混合在一起增强检测能力。对这些数据的稳健统计分析仍然是一个开放的研究问题，还有待确定如何最好地调整偏差。 练习1 现提供三个不同来源的诱导多功能干细胞UMI counts和read counts数据 (Tung et al. 2017) (查看章节 ?? 获得更多关于此数据集的信息) umi_counts &lt;- read.table(&quot;data/tung/molecules.txt&quot;, sep = &quot;\\t&quot;) read_counts &lt;- read.table(&quot;data/tung/reads.txt&quot;, sep = &quot;\\t&quot;) 使用该数据: 绘制捕获效率可变性 确定扩增率：每个UMI的平均reads数目 Determine the amplification rate: average number of reads per UMI. 答案1 # Exercise 1 # Part 1 plot(colSums(umi_counts), colSums(umi_counts &gt; 0), xlab=&quot;Total Molecules Detected&quot;, ylab=&quot;Total Genes Detected&quot;) # Part 2 amp_rate &lt;- sum(read_counts)/sum(umi_counts) amp_rate ## [1] 30.87586 References "],
["intro-r-bioc.html", "5 R/Bioconductor介绍 5.1 安装R包 5.2 安装说明: 5.3 数据类型/类 5.4 Basic data structures 5.5 More information 5.6 Data Types 5.7 Bioconductor, SingleCellExperiment and scater 5.8 An Introduction to ggplot2", " 5 R/Bioconductor介绍 5.1 安装R包 5.1.1 CRAN The Comprehensive R Archive Network CRAN 是最大的R包库。除了成功build和安装之外，对上传R包要求很少，因此文档和相关支持通常很少，弄清楚如何使用这些R包本身成为一个挑战。CRAN是R搜索安装R包的默认库： install.packages(&quot;devtools&quot;) require(&quot;devtools&quot;) 5.1.2 Github Github 并不特定于R，任何状态的任何类型的代码都可以上传。但无法保证上传到github的软件包可以安装，也不保证它声称做的事情。可以使用上述安装的“devtools”软件包直接从github下载和安装R软件包。 devtools::install_github(&quot;tallulandrews/M3Drop&quot;) Github同时也是一个版本控制系统，可以存储任何软件包的多个版本。默认情况下，会安装最新的“master”版本的软件包。如果您想使用旧版本或开发分支，可以使用“ref”参数指定： # different branch devtools::install_github(&quot;tallulandrews/M3D&quot;, ref=&quot;nbumi&quot;) # previous commit devtools::install_github(&quot;tallulandrews/M3Drop&quot;, ref=&quot;434d2da28254acc8de4940c1dc3907ac72973135&quot;) 注意: 确保重新安装M3Drop master分支以便后续课堂使用。 5.1.3 Bioconductor Bioconductor是专门用于生物分析的R包库。它对提交有最严格的要求，包括在各平台上安装，以及完整的文档和教程（称为vignette），解释如何使用包。Bioconductor还鼓励使用标准数据结构/类。 source(&quot;https://bioconductor.org/biocLite.R&quot;) biocLite(&quot;edgeR&quot;) 注意: 某些情况下有必要将上述的“http://” 替换为 “https://” ,这取决于网络连接的安全属性。 Bioconductor还要求作者维护他们的R包，并定期6个月发布更新。 在尝试安装课程所需R包之前，请确保使用最新版本的bioconductor。 source(&quot;https://bioconductor.org/biocLite.R&quot;) biocLite(&quot;BiocUpgrade&quot;) 5.1.4 源码安装 安装包的最后一种方法是直接从源码安装。 在这种情况下，必须下载完整构建的源代码文件，通常是packagename.tar.gz，或克隆github仓并重新build软件包。通常，只有在编辑包或者由于某种原因前一种方法失败时才会这样做。 install.packages(&quot;M3Drop_3.05.00.tar.gz&quot;, type=&quot;source&quot;) 5.2 安装说明: 本课程所需的所有软件包都可以在这里获得。从“RUN Rscript -e”install.packages(‘devtools’)\" “开始，在命令行或者R session运行引号内的命令(移除”RUN\")。注意，某些情况下安装顺序也很重要，请确保按顺序执行。 5.3 数据类型/类 R是一种高级语言，因此底层数据类型通常并不重要。 如果您使用其他语言（如C）直接访问R数据，则需要考虑，但这超出了本课程的范围。 相反，我们将考虑基本数据类型：数值(numeric)，整数(integer)，逻辑(logical)和字符(character)，以及高级数据类“因子”。 使用“class（）”函数检查数据的类型。 除此之外：R还可以将数据存储为“复数(complex)”，但通常这与生物分析无关。 5.3.1 数值 “数值”类型是存储任何数值数据的默认类 - 整数，十进制数，科学计数法中的数字等… x = 1.141 class(x) ## [1] &quot;numeric&quot; y = 42 class(y) ## [1] &quot;numeric&quot; z = 6.02e23 class(z) ## [1] &quot;numeric&quot; 即使R有一个“整数”类型，42可以更有效地存储为整数，默认是将其存储为“数值”类型。 如果我们想要将42存储为整数，我们必须强制类型转换： y = as.integer(42) class(y) ## [1] &quot;integer&quot; 强制R将数据存储为特定类，如果我们的数据与该类不兼容，它仍将执行此操作，但数据将转换为NA： as.numeric(&quot;H&quot;) ## Warning: 强制改变过程中产生了NA ## [1] NA 上述将“字符”数据强制转换为无意义的数值，因此触发（“抛出”）警告消息。 由于这只是一个警告信息，R将继续执行脚本/函数中的后续命令，而“错误”将导致R停止执行。 5.3.2 字符/字符串 “character”类型存储各种文本数据。 编程约定将包含多个字母的数据称为“字符串”，因此大多数作用于字符数据的R函数将数据称为“字符串”，并且通常在其名称中包含“str”或“string”。字符串通过双引号标识，而变量/函数名称则不是： x = 5 a = &quot;x&quot; # character &quot;x&quot; a ## [1] &quot;x&quot; b = x # variable x b ## [1] 5 In addition to standard alphanumeric characters, strings can also store various special characters. Special characters are identified using a backlash followed by a single character, the most relevant are the special character for tab : \\t and new line : \\n. To demonstrate the these special characters lets concatenate (cat) together two strings with these characters separating (sep) them: cat(&quot;Hello&quot;, &quot;World&quot;, sep= &quot; &quot;) ## Hello World cat(&quot;Hello&quot;, &quot;World&quot;, sep= &quot;\\t&quot;) ## Hello World cat(&quot;Hello&quot;, &quot;World&quot;, sep= &quot;\\n&quot;) ## Hello ## World Note that special characters work differently in different functions. For instance the paste function does the same thing as cat but does not recognize special characters. paste(&quot;Hello&quot;, &quot;World&quot;, sep= &quot; &quot;) ## [1] &quot;Hello World&quot; paste(&quot;Hello&quot;, &quot;World&quot;, sep= &quot;\\t&quot;) ## [1] &quot;Hello\\tWorld&quot; paste(&quot;Hello&quot;, &quot;World&quot;, sep= &quot;\\n&quot;) ## [1] &quot;Hello\\nWorld&quot; Single or double backslash is also used as an escape character to turn off special characters or allow quotation marks to be included in strings: cat(&quot;This \\&quot;string\\&quot; contains quotation marks.&quot;) ## This &quot;string&quot; contains quotation marks. Special characters are generally only used in pattern matching, and reading/writing data to files. For instance this is how you would read a tab-separated file into R. dat = read.delim(&quot;file.tsv&quot;, sep=&quot;\\t&quot;) Another special type of character data are colours. Colours can be specified in three main ways: by name from those available, by red, green, blue values using the rgb function, and by hue (colour), saturation (colour vs white) and value (colour/white vs black) using the hsv function. By default rgb and hsv expect three values in 0-1 with an optional fourth value for transparency. Alternatively, sets of predetermined colours with useful properties can be loaded from many different packages with RColorBrewer being one of the most popular. reds = c(&quot;red&quot;, rgb(1,0,0), hsv(0, 1, 1)) reds ## [1] &quot;red&quot; &quot;#FF0000&quot; &quot;#FF0000&quot; barplot(c(1,1,1), col=reds, names=c(&quot;by_name&quot;, &quot;by_rgb&quot;, &quot;by_hsv&quot;)) 5.3.3 Logical The logical class stores boolean truth values, i.e. TRUE and FALSE. It is used for storing the results of logical operations and conditional statements will be coerced to this class. Most other data-types can be coerced to boolean without triggering (or “throwing”) error messages, which may cause unexpected behaviour. x = TRUE class(x) ## [1] &quot;logical&quot; y = &quot;T&quot; as.logical(y) ## [1] TRUE z = 5 as.logical(z) ## [1] TRUE x = FALSE class(x) ## [1] &quot;logical&quot; y = &quot;F&quot; as.logical(y) ## [1] FALSE z = 0 as.logical(z) ## [1] FALSE Exercise 1 Experiment with other character and numeric values, which are coerced to TRUE or FALSE? which are coerced to neither? Do you ever throw a warning/error message? 5.3.4 Factors String/Character data is very memory inefficient to store, each letter generally requires the same amount of memory as any integer. Thus when storing a vector of strings with repeated elements it is more efficient assign each element to an integer and store the vector as integers and an additional string-to-integer association table. Thus, by default R will read in text columns of a data table as factors. str_vector = c(&quot;Apple&quot;, &quot;Apple&quot;, &quot;Banana&quot;, &quot;Banana&quot;, &quot;Banana&quot;, &quot;Carrot&quot;, &quot;Carrot&quot;, &quot;Apple&quot;, &quot;Banana&quot;) factored_vector = factor(str_vector) factored_vector ## [1] Apple Apple Banana Banana Banana Carrot Carrot Apple Banana ## Levels: Apple Banana Carrot as.numeric(factored_vector) ## [1] 1 1 2 2 2 3 3 1 2 The double nature of factors can cause some unintuitive behaviour. E.g. joining two factors together will convert them to the numeric form and the original strings will be lost. c(factored_vector, factored_vector) ## [1] 1 1 2 2 2 3 3 1 2 1 1 2 2 2 3 3 1 2 Likewise if due to formatting issues numeric data is mistakenly interpretted as strings, then you must convert the factor back to strings before coercing to numeric values: x = c(&quot;20&quot;, &quot;25&quot;, &quot;23&quot;, &quot;38&quot;, &quot;20&quot;, &quot;40&quot;, &quot;25&quot;, &quot;30&quot;) x = factor(x) as.numeric(x) ## [1] 1 3 2 5 1 6 3 4 as.numeric(as.character(x)) ## [1] 20 25 23 38 20 40 25 30 To make R read text as character data instead of factors set the environment option stringsAsFactors=FALSE. This must be done at the start of each R session. options(stringsAsFactors=FALSE) Exercise How would you use factors to create a vector of colours for an arbitrarily long vector of fruits like str_vector above? Answer 5.3.5 Checking class/type We recommend checking your data is of the correct class after reading from files: x = 1.4 is.numeric(x) ## [1] TRUE is.character(x) ## [1] FALSE is.logical(x) ## [1] FALSE is.factor(x) ## [1] FALSE 5.4 Basic data structures So far we have only looked at single values and vectors. Vectors are the simplest data structure in R. They are a 1-dimensional array of data all of the same type. If the input when creating a vector is of different types it will be coerced to the data-type that is most consistent with the data. x = c(&quot;Hello&quot;, 5, TRUE) x ## [1] &quot;Hello&quot; &quot;5&quot; &quot;TRUE&quot; class(x) ## [1] &quot;character&quot; Here we tried to put character, numeric and logical data into a single vector so all the values were coerced to character data. A matrix is the two dimensional version of a vector, it also requires all data to be of the same type. If we combine a character vector and a numeric vector into a matrix, all the data will be coerced to characters: x = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;) y = c(1, 2, 3) class(x) ## [1] &quot;character&quot; class(y) ## [1] &quot;numeric&quot; m = cbind(x, y) m ## x y ## [1,] &quot;A&quot; &quot;1&quot; ## [2,] &quot;B&quot; &quot;2&quot; ## [3,] &quot;C&quot; &quot;3&quot; The quotation marks indicate that the numeric vector has been coerced to characters. Alternatively, to store data with columns of different data-types we can use a dataframe. z = data.frame(x, y) z ## x y ## 1 A 1 ## 2 B 2 ## 3 C 3 class(z[,1]) ## [1] &quot;character&quot; class(z[,2]) ## [1] &quot;numeric&quot; If you have set stringsAsFactors=FALSE as above you will find the first column remains characters, otherwise it will be automatically converted to a factor. options(stringsAsFactors=TRUE) z = data.frame(x, y) class(z[,1]) ## [1] &quot;factor&quot; Another difference between matrices and dataframes is the ability to select columns using the $ operator: m$x # throws an error z$x # ok The final basic data structure is the list. Lists allow data of different types and different lengths to be stored in a single object. Each element of a list can be any other R object : data of any type, any data structure, even other lists or functions. l = list(m, z) ll = list(sublist=l, a_matrix=m, numeric_value=42, this_string=&quot;Hello World&quot;, even_a_function=cbind) ll ## $sublist ## $sublist[[1]] ## x y ## [1,] &quot;A&quot; &quot;1&quot; ## [2,] &quot;B&quot; &quot;2&quot; ## [3,] &quot;C&quot; &quot;3&quot; ## ## $sublist[[2]] ## x y ## 1 A 1 ## 2 B 2 ## 3 C 3 ## ## ## $a_matrix ## x y ## [1,] &quot;A&quot; &quot;1&quot; ## [2,] &quot;B&quot; &quot;2&quot; ## [3,] &quot;C&quot; &quot;3&quot; ## ## $numeric_value ## [1] 42 ## ## $this_string ## [1] &quot;Hello World&quot; ## ## $even_a_function ## function (..., deparse.level = 1) ## .Internal(cbind(deparse.level, ...)) ## &lt;bytecode: 0x0000000008278de8&gt; ## &lt;environment: namespace:base&gt; Lists are most commonly used when returning a large number of results from a function that do not fit into any of the previous data structures. 5.5 More information You can get more information about any R commands relevant to these datatypes using by typing ?function in an interactive session. 5.6 Data Types 5.6.1 What is Tidy Data? Tidy data is a concept largely defined by Hadley Wickham (Wickham 2014). Tidy data has the following three characteristics: Each variable has its own column. Each observation has its own row. Each value has its own cell. Here is an example of some tidy data: ## Students Subject Years Score ## 1 Mark Maths 1 5 ## 2 Jane Biology 2 6 ## 3 Mohammed Physics 3 4 ## 4 Tom Maths 2 7 ## 5 Celia Computing 3 9 Here is an example of some untidy data: ## Students Sport Category Counts ## 1 Matt Tennis Wins 0 ## 2 Matt Tennis Losses 1 ## 3 Ellie Rugby Wins 3 ## 4 Ellie Rugby Losses 2 ## 5 Tim Football Wins 1 ## 6 Tim Football Losses 4 ## 7 Louise Swimming Wins 2 ## 8 Louise Swimming Losses 2 ## 9 Kelly Running Wins 5 ## 10 Kelly Running Losses 1 Task 1: In what ways is the untidy data not tidy? How could we make the untidy data tidy? Tidy data is generally easier to work with than untidy data, especially if you are working with packages such as ggplot. Fortunately, packages are available to make untidy data tidy. Today we will explore a few of the functions available in the tidyr package which can be used to make untidy data tidy. If you are interested in finding out more about tidying data, we recommend reading “R for Data Science”, by Garrett Grolemund and Hadley Wickham. An electronic copy is available here: http://r4ds.had.co.nz/ The untidy data above is untidy because two variables (Wins and Losses) are stored in one column (Category). This is a common way in which data can be untidy. To tidy this data, we need to make Wins and Losses into columns, and store the values in Counts in these columns. Fortunately, there is a function from the tidyverse packages to perform this operation. The function is called spread, and it takes two arguments, key and value. You should pass the name of the column which contains multiple variables to key, and pass the name of the column which contains values from multiple variables to value. For example: library(tidyverse) sports&lt;-data.frame(Students=c(&quot;Matt&quot;, &quot;Matt&quot;, &quot;Ellie&quot;, &quot;Ellie&quot;, &quot;Tim&quot;, &quot;Tim&quot;, &quot;Louise&quot;, &quot;Louise&quot;, &quot;Kelly&quot;, &quot;Kelly&quot;), Sport=c(&quot;Tennis&quot;,&quot;Tennis&quot;, &quot;Rugby&quot;, &quot;Rugby&quot;,&quot;Football&quot;, &quot;Football&quot;,&quot;Swimming&quot;,&quot;Swimming&quot;, &quot;Running&quot;, &quot;Running&quot;), Category=c(&quot;Wins&quot;, &quot;Losses&quot;, &quot;Wins&quot;, &quot;Losses&quot;, &quot;Wins&quot;, &quot;Losses&quot;, &quot;Wins&quot;, &quot;Losses&quot;, &quot;Wins&quot;, &quot;Losses&quot;), Counts=c(0,1,3,2,1,4,2,2,5,1)) sports ## Students Sport Category Counts ## 1 Matt Tennis Wins 0 ## 2 Matt Tennis Losses 1 ## 3 Ellie Rugby Wins 3 ## 4 Ellie Rugby Losses 2 ## 5 Tim Football Wins 1 ## 6 Tim Football Losses 4 ## 7 Louise Swimming Wins 2 ## 8 Louise Swimming Losses 2 ## 9 Kelly Running Wins 5 ## 10 Kelly Running Losses 1 spread(sports, key=Category, value=Counts) ## Students Sport Losses Wins ## 1 Ellie Rugby 2 3 ## 2 Kelly Running 1 5 ## 3 Louise Swimming 2 2 ## 4 Matt Tennis 1 0 ## 5 Tim Football 4 1 Task 2: The dataframe foods defined below is untidy. Work out why and use spread() to tidy it foods&lt;-data.frame(student=c(&quot;Antoinette&quot;,&quot;Antoinette&quot;,&quot;Taylor&quot;, &quot;Taylor&quot;, &quot;Alexa&quot;, &quot;Alexa&quot;), Category=c(&quot;Dinner&quot;, &quot;Dessert&quot;, &quot;Dinner&quot;, &quot;Dessert&quot;, &quot;Dinner&quot;,&quot;Dessert&quot;), Frequency=c(3,1,4,5,2,1)) The other common way in which data can be untidy is if the columns are values instead of variables. For example, the dataframe below shows the percentages some students got in tests they did in May and June. The data is untidy because the columns May and June are values, not variables. percentages&lt;-data.frame(student=c(&quot;Alejandro&quot;, &quot;Pietro&quot;, &quot;Jane&quot;), &quot;May&quot;=c(90,12,45), &quot;June&quot;=c(80,30,100)) Fortunately, there is a function in the tidyverse packages to deal with this problem too. gather() takes the names of the columns which are values, the key and the value as arguments. This time, the key is the name of the variable with values as column names, and the value is the name of the variable with values spread over multiple columns. Ie: gather(percentages, &quot;May&quot;, &quot;June&quot;, key=&quot;Month&quot;, value = &quot;Percentage&quot;) ## student Month Percentage ## 1 Alejandro May 90 ## 2 Pietro May 12 ## 3 Jane May 45 ## 4 Alejandro June 80 ## 5 Pietro June 30 ## 6 Jane June 100 These examples don’t have much to do with single-cell RNA-seq analysis, but are designed to help illustrate the features of tidy and untidy data. You will find it much easier to analyse your single-cell RNA-seq data if your data is stored in a tidy format. Fortunately, the data structures we commonly use to facilitate single-cell RNA-seq analysis usually encourage store your data in a tidy manner. 5.6.2 What is Rich Data? If you google ‘rich data’, you will find lots of different definitions for this term. In this course, we will use ‘rich data’ to mean data which is generated by combining information from multiple sources. For example, you could make rich data by creating an object in R which contains a matrix of gene expression values across the cells in your single-cell RNA-seq experiment, but also information about how the experiment was performed. Objects of the SingleCellExperiment class, which we will discuss below, are an example of rich data. 5.7 Bioconductor, SingleCellExperiment and scater 5.7.1 Bioconductor From Wikipedia: Bioconductor is a free, open source and open development software project for the analysis and comprehension of genomic data generated by wet lab experiments in molecular biology. Bioconductor is based primarily on the statistical R programming language, but does contain contributions in other programming languages. It has two releases each year that follow the semiannual releases of R. At any one time there is a release version, which corresponds to the released version of R, and a development version, which corresponds to the development version of R. Most users will find the release version appropriate for their needs. We strongly recommend all new comers and even experienced high-throughput data analysts to use well developed and maintained Bioconductor methods and classes. 5.7.2 SingleCellExperiment class SingleCellExperiment (SCE) is a S4 class for storing data from single-cell experiments. This includes specialized methods to store and retrieve spike-in information, dimensionality reduction coordinates and size factors for each cell, along with the usual metadata for genes and libraries. In practice, an object of this class can be created using its constructor: library(SingleCellExperiment) counts &lt;- matrix(rpois(100, lambda = 10), ncol=10, nrow=10) rownames(counts) &lt;- paste(&quot;gene&quot;, 1:10, sep = &quot;&quot;) colnames(counts) &lt;- paste(&quot;cell&quot;, 1:10, sep = &quot;&quot;) sce &lt;- SingleCellExperiment( assays = list(counts = counts), rowData = data.frame(gene_names = paste(&quot;gene_name&quot;, 1:10, sep = &quot;&quot;)), colData = data.frame(cell_names = paste(&quot;cell_name&quot;, 1:10, sep = &quot;&quot;)) ) sce ## class: SingleCellExperiment ## dim: 10 10 ## metadata(0): ## assays(1): counts ## rownames(10): gene1 gene2 ... gene9 gene10 ## rowData names(1): gene_names ## colnames(10): cell1 cell2 ... cell9 cell10 ## colData names(1): cell_names ## reducedDimNames(0): ## spikeNames(0): In the SingleCellExperiment, users can assign arbitrary names to entries of assays. To assist interoperability between packages, some suggestions for what the names should be for particular types of data are provided by the authors: counts: Raw count data, e.g., number of reads or transcripts for a particular gene. normcounts: Normalized values on the same scale as the original counts. For example, counts divided by cell-specific size factors that are centred at unity. logcounts: Log-transformed counts or count-like values. In most cases, this will be defined as log-transformed normcounts, e.g., using log base 2 and a pseudo-count of 1. cpm: Counts-per-million. This is the read count for each gene in each cell, divided by the library size of each cell in millions. tpm: Transcripts-per-million. This is the number of transcripts for each gene in each cell, divided by the total number of transcripts in that cell (in millions). Each of these suggested names has an appropriate getter/setter method for convenient manipulation of the SingleCellExperiment. For example, we can take the (very specifically named) counts slot, normalise it and assign it to normcounts instead: normcounts(sce) &lt;- log2(counts(sce) + 1) sce ## class: SingleCellExperiment ## dim: 10 10 ## metadata(0): ## assays(2): counts normcounts ## rownames(10): gene1 gene2 ... gene9 gene10 ## rowData names(1): gene_names ## colnames(10): cell1 cell2 ... cell9 cell10 ## colData names(1): cell_names ## reducedDimNames(0): ## spikeNames(0): dim(normcounts(sce)) ## [1] 10 10 head(normcounts(sce)) ## cell1 cell2 cell3 cell4 cell5 cell6 cell7 ## gene1 3.169925 3.169925 2.000000 2.584963 2.584963 3.321928 3.584963 ## gene2 3.459432 1.584963 3.584963 3.807355 3.700440 3.700440 3.000000 ## gene3 3.000000 3.169925 3.807355 3.169925 3.321928 3.321928 3.321928 ## gene4 3.584963 3.459432 3.000000 3.807355 3.700440 3.700440 3.700440 ## gene5 3.906891 3.000000 3.169925 3.321928 3.584963 3.459432 3.807355 ## gene6 3.700440 3.700440 3.584963 4.000000 3.169925 3.000000 3.459432 ## cell8 cell9 cell10 ## gene1 3.321928 3.807355 2.807355 ## gene2 3.807355 3.700440 4.000000 ## gene3 2.584963 4.000000 3.700440 ## gene4 3.169925 3.584963 3.700440 ## gene5 3.807355 2.584963 3.584963 ## gene6 3.321928 3.459432 4.000000 5.7.3 scater package scater is a R package for single-cell RNA-seq analysis (McCarthy et al. 2017). The package contains several useful methods for quality control, visualisation and pre-processing of data prior to further downstream analysis. scater features the following functionality: Automated computation of QC metrics Transcript quantification from read data with pseudo-alignment Data format standardisation Rich visualizations for exploratory analysis Seamless integration into the Bioconductor universe Simple normalisation methods We highly recommend to use scater for all single-cell RNA-seq analyses and scater is the basis of the first part of the course. As illustrated in the figure below, scater will help you with quality control, filtering and normalization of your expression matrix following mapping and alignment. Keep in mind that this figure represents the original version of scater where an SCESet class was used. In the newest version this figure is still correct, except that SCESet can be substituted with the SingleCellExperiment class. 5.8 An Introduction to ggplot2 5.8.1 What is ggplot2? ggplot2 is an R package designed by Hadley Wickham which facilitates data plotting. In this lab, we will touch briefly on some of the features of the package. If you would like to learn more about how to use ggplot2, we would recommend reading “ggplot2 Elegant graphics for data analysis”, by Hadley Wickham. 5.8.2 Principles of ggplot2 Your data must be a dataframe if you want to plot it using ggplot2. Use the aes mapping function to specify how variables in the dataframe map to features on your plot Use geoms to specify how your data should be represented on your graph eg. as a scatterplot, a barplot, a boxplot etc. 5.8.3 Using the aes mapping function The aes function specifies how variables in your dataframe map to features on your plot. To understand how this works, let’s look at an example: library(ggplot2) library(tidyverse) set.seed(1) counts &lt;- as.data.frame(matrix(rpois(100, lambda = 10), ncol=10, nrow=10)) Gene_ids &lt;- paste(&quot;gene&quot;, 1:10, sep = &quot;&quot;) colnames(counts) &lt;- paste(&quot;cell&quot;, 1:10, sep = &quot;&quot;) counts&lt;-data.frame(Gene_ids, counts) counts ## Gene_ids cell1 cell2 cell3 cell4 cell5 cell6 cell7 cell8 cell9 cell10 ## 1 gene1 8 8 3 5 5 9 11 9 13 6 ## 2 gene2 10 2 11 13 12 12 7 13 12 15 ## 3 gene3 7 8 13 8 9 9 9 5 15 12 ## 4 gene4 11 10 7 13 12 12 12 8 11 12 ## 5 gene5 14 7 8 9 11 10 13 13 5 11 ## 6 gene6 12 12 11 15 8 7 10 9 10 15 ## 7 gene7 11 11 14 11 11 5 9 13 13 7 ## 8 gene8 9 12 9 8 6 14 7 12 12 10 ## 9 gene9 14 12 11 7 10 10 8 14 7 10 ## 10 gene10 11 10 9 7 11 16 8 7 7 4 ggplot(data = counts, mapping = aes(x = cell1, y = cell2)) Let’s take a closer look at the final command, ggplot(data = counts, mapping = aes(x = cell1, y = cell2)). ggplot() initialises a ggplot object and takes the arguments data and mapping. We pass our dataframe of counts to data and use the aes() function to specify that we would like to use the variable cell1 as our x variable and the variable cell2 as our y variable. Task 1: Modify the command above to initialise a ggplot object where cell10 is the x variable and cell8 is the y variable. Clearly, the plots we have just created are not very informative because no data is displayed on them. To display data, we will need to use geoms. 5.8.4 Geoms We can use geoms to specify how we would like data to be displayed on our graphs. For example, our choice of geom could specify that we would like our data to be displayed as a scatterplot, a barplot or a boxplot. Let’s see how our graph would look as a scatterplot. ggplot(data = counts, mapping = aes(x = cell1, y = cell2)) + geom_point() Now we can see that there doesn’t seem to be any correlation between gene expression in cell1 and cell2. Given we generated counts randomly, this isn’t too surprising. Task 2: Modify the command above to create a line plot. Hint: execute ?ggplot and scroll down the help page. At the bottom is a link to the ggplot package index. Scroll through the index until you find the geom options. 5.8.5 Plotting data from more than 2 cells So far we’ve been considering the gene counts from 2 of the cells in our dataframe. But there are actually 10 cells in our dataframe and it would be nice to compare all of them. What if we wanted to plot data from all 10 cells at the same time? At the moment we can’t do this because we are treating each individual cell as a variable and assigning that variable to either the x or the y axis. We could create a 10 dimensional graph to plot data from all 10 cells on, but this is a) not possible to do with ggplot and b) not very easy to interpret. What we could do instead is to tidy our data so that we had one variable representing cell ID and another variable representing gene counts, and plot those against each other. In code, this would look like: counts&lt;-gather(counts, colnames(counts)[2:11], key = &#39;Cell_ID&#39;, value=&#39;Counts&#39;) head(counts) ## Gene_ids Cell_ID Counts ## 1 gene1 cell1 8 ## 2 gene2 cell1 10 ## 3 gene3 cell1 7 ## 4 gene4 cell1 11 ## 5 gene5 cell1 14 ## 6 gene6 cell1 12 Essentially, the problem before was that our data was not tidy because one variable (Cell_ID) was spread over multiple columns. Now that we’ve fixed this problem, it is much easier for us to plot data from all 10 cells on one graph. ggplot(counts,aes(x=Cell_ID, y=Counts)) + geom_boxplot() Task 3: Use the updated counts dataframe to plot a barplot with Cell_ID as the x variable and Counts as the y variable. Hint: you may find it helpful to read ?geom_bar. Task 4: Use the updated counts dataframe to plot a scatterplot with Gene_ids as the x variable and Counts as the y variable. 5.8.6 Plotting heatmaps A common method for visualising gene expression data is with a heatmap. Here we will use the R package pheatmap to perform this analysis with some gene expression data we will name test. library(pheatmap) ## Warning: 程辑包&#39;pheatmap&#39;是用R版本3.5.2 来建造的 set.seed(2) test = matrix(rnorm(200), 20, 10) test[1:10, seq(1, 10, 2)] = test[1:10, seq(1, 10, 2)] + 3 test[11:20, seq(2, 10, 2)] = test[11:20, seq(2, 10, 2)] + 2 test[15:20, seq(2, 10, 2)] = test[15:20, seq(2, 10, 2)] + 4 colnames(test) = paste(&quot;Cell&quot;, 1:10, sep = &quot;&quot;) rownames(test) = paste(&quot;Gene&quot;, 1:20, sep = &quot;&quot;) pheatmap(test) Let’s take a moment to work out what this graphic is showing us. Each row represents a gene and each column represents a cell. How highly expressed each gene is in each cell is represented by the colour of the corresponding box. For example, we can tell from this plot that gene18 is highly expressed in cell10 but lowly expressed in cell1. This plot also gives us information on the results of a clustering algorithm. In general, clustering algorithms aim to split datapoints (eg.cells) into groups whose members are more alike one another than they are alike the rest of the datapoints. The trees drawn on the top and left hand sides of the graph are the results of clustering algorithms and enable us to see, for example, that cells 4,8,2,6 and 10 are more alike one another than they are alike cells 7,3,5,1 and 9. The tree on the left hand side of the graph represents the results of a clustering algorithm applied to the genes in our dataset. If we look closely at the trees, we can see that eventually they have the same number of branches as there are cells and genes. In other words, the total number of cell clusters is the same as the total number of cells, and the total number of gene clusters is the same as the total number of genes. Clearly, this is not very informative, and will become impractical when we are looking at more than 10 cells and 20 genes. Fortunately, we can set the number of clusters we see on the plot. Let’s try setting the number of gene clusters to 2: pheatmap(test, kmeans_k = 2) Now we can see that the genes fall into two clusters - a cluster of 8 genes which are upregulated in cells 2, 10, 6, 4 and 8 relative to the other cells and a cluster of 12 genes which are downregulated in cells 2, 10, 6, 4 and 8 relative to the other cells. Task 5: Try setting the number of clusters to 3. Which number of clusters do you think is more informative? 5.8.7 Principal Component Analysis Principal component analysis (PCA) is a statistical procedure that uses a transformation to convert a set of observations into a set of values of linearly uncorrelated variables called principal components. The transformation is carried out so that the first principle component accounts for as much of the variability in the data as possible, and each following principle component accounts for the greatest amount of variance possible under the contraint that it must be orthogonal to the previous components. PCA plots are a good way to get an overview of your data, and can sometimes help identify confounders which explain a high amount of the variability in your data. We will investigate how we can use PCA plots in single-cell RNA-seq analysis in more depth in a future lab, here the aim is to give you an overview of what PCA plots are and how they are generated. Let’s make a PCA plot for our test data. We can use the ggfortify package to let ggplot know how to interpret principle components. library(ggfortify) ## Warning: 程辑包&#39;ggfortify&#39;是用R版本3.5.3 来建造的 Principal_Components&lt;-prcomp(test) autoplot(Principal_Components, label=TRUE) Task 6: Compare your clusters to the pheatmap clusters. Are they related? (Hint: have a look at the gene tree for the first pheatmap we plotted) Task 7: Produce a heatmap and PCA plot for counts (below): set.seed(1) counts &lt;- as.data.frame(matrix(rpois(100, lambda = 10), ncol=10, nrow=10)) rownames(counts) &lt;- paste(&quot;gene&quot;, 1:10, sep = &quot;&quot;) colnames(counts) &lt;- paste(&quot;cell&quot;, 1:10, sep = &quot;&quot;) References "],
["tabula-muris.html", "6 Tabula Muris 6.1 Introduction 6.2 Downloading the data 6.3 Reading the data (Smartseq2) 6.4 Building a scater object 6.5 Reading the data (10X) 6.6 Building a scater object 6.7 Advanced Exercise", " 6 Tabula Muris 6.1 Introduction To give you hands-on experience analyzing from start to finish a single-cell RNASeq dataset we will be using as an example, data from the Tabula Muris initial release. The Tabula Muris is an international collaboration with the aim to profile every cell-type in the mouse using a standardized method. They combine highthroughput but low-coverage 10X data with lower throughput but high-coverage FACS-sorted cells + Smartseq2. The initial release of the data (20 Dec 2017), contain almost 100,000 cells across 20 different tissues/organs. You have been assigned one of these tissues as an example to work on over this course, and on Friday each person will have 3 minutes to present the result for their tissue. 6.2 Downloading the data Unlike most single-cell RNASeq data Tabula Muris has release their data through the figshare platform rather than uploading it to GEO or ArrayExpress. You can find the data by using the doi’s in their paper : 10.6084/m9.figshare.5715040 for FACS/Smartseq2 and 10.6084/m9.figshare.5715025 for 10X data. The data can be downloaded manually by clinking the doi links or by using the command-line commands below: Terminal-based download of FACS data: wget https://ndownloader.figshare.com/files/10038307 unzip 10038307 wget https://ndownloader.figshare.com/files/10038310 mv 10038310 FACS_metadata.csv wget https://ndownloader.figshare.com/files/10039267 mv 10039267 FACS_annotations.csv Terminal-based download of 10X data: wget https://ndownloader.figshare.com/files/10038325 unzip 10038325 wget https://ndownloader.figshare.com/files/10038328 mv 10038328 droplet_metadata.csv wget https://ndownloader.figshare.com/files/10039264 mv 10039264 droplet_annotation.csv Note if you download the data by hand you should unzip &amp; rename the files as above before continuing. You should now have two folders : “FACS” and “droplet” and one annotation and metadata file for each. To inspect these files you can use the head to see the top few lines of the text files (Press “q” to exit): head -n 10 data/droplet/droplet_metadata.csv ## bash.exe: warning: could not find /tmp, please create! ## bash.exe: warning: could not find /tmp, please create! ## channel,mouse.id,tissue,subtissue,mouse.sex ## 10X_P4_0,3-M-8,Tongue,NA,M ## 10X_P4_1,3-M-9,Tongue,NA,M ## 10X_P4_2,3-M-8/9,Liver,hepatocytes,M ## 10X_P4_3,3-M-8,Bladder,NA,M ## 10X_P4_4,3-M-9,Bladder,NA,M ## 10X_P4_5,3-M-8,Kidney,NA,M ## 10X_P4_6,3-M-9,Kidney,NA,M ## 10X_P4_7,3-M-8,Spleen,NA,M ## 10X_P7_0,3-F-56,Liver,NA,F You can also check the number of rows in each file using: wc -l data/droplet/droplet_annotation.csv ## bash.exe: warning: could not find /tmp, please create! ## bash.exe: warning: could not find /tmp, please create! ## 54838 data/droplet/droplet_annotation.csv Exercise How many cells do we have annotations for from FACS? from 10X? Answer FACS : 54,838 cells Droplet : 42,193 cells 6.3 Reading the data (Smartseq2) We can now read in the relevant count matrix from the comma-separated file. Then inspect the resulting dataframe: dat = read.delim(&quot;data/FACS/Kidney-counts.csv&quot;, sep=&quot;,&quot;, header=TRUE) dat[1:5,1:5] ## X A14.MAA000545.3_8_M.1.1 E1.MAA000545.3_8_M.1.1 ## 1 0610005C13Rik 0 0 ## 2 0610007C21Rik 1 0 ## 3 0610007L01Rik 0 0 ## 4 0610007N19Rik 0 0 ## 5 0610007P08Rik 0 0 ## M4.MAA000545.3_8_M.1.1 O21.MAA000545.3_8_M.1.1 ## 1 0 0 ## 2 0 0 ## 3 0 0 ## 4 0 0 ## 5 0 0 We can see that the first column in the dataframe is the gene names, so first we move these to the rownames so we have a numeric matrix: dim(dat) ## [1] 23433 866 rownames(dat) &lt;- dat[,1] dat &lt;- dat[,-1] Since this is a Smartseq2 dataset it may contain spike-ins so lets check: rownames(dat)[grep(&quot;^ERCC-&quot;, rownames(dat))] ## [1] &quot;ERCC-00002&quot; &quot;ERCC-00003&quot; &quot;ERCC-00004&quot; &quot;ERCC-00009&quot; &quot;ERCC-00012&quot; ## [6] &quot;ERCC-00013&quot; &quot;ERCC-00014&quot; &quot;ERCC-00016&quot; &quot;ERCC-00017&quot; &quot;ERCC-00019&quot; ## [11] &quot;ERCC-00022&quot; &quot;ERCC-00024&quot; &quot;ERCC-00025&quot; &quot;ERCC-00028&quot; &quot;ERCC-00031&quot; ## [16] &quot;ERCC-00033&quot; &quot;ERCC-00034&quot; &quot;ERCC-00035&quot; &quot;ERCC-00039&quot; &quot;ERCC-00040&quot; ## [21] &quot;ERCC-00041&quot; &quot;ERCC-00042&quot; &quot;ERCC-00043&quot; &quot;ERCC-00044&quot; &quot;ERCC-00046&quot; ## [26] &quot;ERCC-00048&quot; &quot;ERCC-00051&quot; &quot;ERCC-00053&quot; &quot;ERCC-00054&quot; &quot;ERCC-00057&quot; ## [31] &quot;ERCC-00058&quot; &quot;ERCC-00059&quot; &quot;ERCC-00060&quot; &quot;ERCC-00061&quot; &quot;ERCC-00062&quot; ## [36] &quot;ERCC-00067&quot; &quot;ERCC-00069&quot; &quot;ERCC-00071&quot; &quot;ERCC-00073&quot; &quot;ERCC-00074&quot; ## [41] &quot;ERCC-00075&quot; &quot;ERCC-00076&quot; &quot;ERCC-00077&quot; &quot;ERCC-00078&quot; &quot;ERCC-00079&quot; ## [46] &quot;ERCC-00081&quot; &quot;ERCC-00083&quot; &quot;ERCC-00084&quot; &quot;ERCC-00085&quot; &quot;ERCC-00086&quot; ## [51] &quot;ERCC-00092&quot; &quot;ERCC-00095&quot; &quot;ERCC-00096&quot; &quot;ERCC-00097&quot; &quot;ERCC-00098&quot; ## [56] &quot;ERCC-00099&quot; &quot;ERCC-00104&quot; &quot;ERCC-00108&quot; &quot;ERCC-00109&quot; &quot;ERCC-00111&quot; ## [61] &quot;ERCC-00112&quot; &quot;ERCC-00113&quot; &quot;ERCC-00116&quot; &quot;ERCC-00117&quot; &quot;ERCC-00120&quot; ## [66] &quot;ERCC-00123&quot; &quot;ERCC-00126&quot; &quot;ERCC-00130&quot; &quot;ERCC-00131&quot; &quot;ERCC-00134&quot; ## [71] &quot;ERCC-00136&quot; &quot;ERCC-00137&quot; &quot;ERCC-00138&quot; &quot;ERCC-00142&quot; &quot;ERCC-00143&quot; ## [76] &quot;ERCC-00144&quot; &quot;ERCC-00145&quot; &quot;ERCC-00147&quot; &quot;ERCC-00148&quot; &quot;ERCC-00150&quot; ## [81] &quot;ERCC-00154&quot; &quot;ERCC-00156&quot; &quot;ERCC-00157&quot; &quot;ERCC-00158&quot; &quot;ERCC-00160&quot; ## [86] &quot;ERCC-00162&quot; &quot;ERCC-00163&quot; &quot;ERCC-00164&quot; &quot;ERCC-00165&quot; &quot;ERCC-00168&quot; ## [91] &quot;ERCC-00170&quot; &quot;ERCC-00171&quot; Now we can extract much of the metadata for this data from the column names: cellIDs &lt;- colnames(dat) cell_info &lt;- strsplit(cellIDs, &quot;\\\\.&quot;) Well &lt;- lapply(cell_info, function(x){x[1]}) Well &lt;- unlist(Well) Plate &lt;- unlist(lapply(cell_info, function(x){x[2]})) Mouse &lt;- unlist(lapply(cell_info, function(x){x[3]})) We can check the distributions of each of these metadata classifications: summary(factor(Mouse)) ## 3_10_M 3_11_M 3_38_F 3_39_F 3_8_M 3_9_M ## 104 196 237 169 82 77 We can also check if any technical factors are confounded: table(Mouse, Plate) ## Plate ## Mouse B001717 B002775 MAA000545 MAA000752 MAA000801 MAA000922 ## 3_10_M 0 0 0 104 0 0 ## 3_11_M 0 0 0 0 196 0 ## 3_38_F 237 0 0 0 0 0 ## 3_39_F 0 169 0 0 0 0 ## 3_8_M 0 0 82 0 0 0 ## 3_9_M 0 0 0 0 0 77 Lastly we will read the computationally inferred cell-type annotation and match them to the cell in our expression matrix: ann &lt;- read.table(&quot;data/FACS/FACS_annotations.csv&quot;, sep=&quot;,&quot;, header=TRUE) ann &lt;- ann[match(cellIDs, ann[,1]),] celltype &lt;- ann[,3] 6.4 Building a scater object To create a SingleCellExperiment object we must put together all the cell annotations into a single dataframe, since the experimental batch (pcr plate) is completely confounded with donor mouse we will only keep one of them. library(&quot;SingleCellExperiment&quot;) library(&quot;scater&quot;) cell_anns &lt;- data.frame(mouse = Mouse, well=Well, type=celltype) rownames(cell_anns) &lt;- colnames(dat) sceset &lt;- SingleCellExperiment(assays = list(counts = as.matrix(dat)), colData=cell_anns) Finally if the dataset contains spike-ins we a hidden variable in the SingleCellExperiment object to track them: isSpike(sceset, &quot;ERCC&quot;) &lt;- grepl(&quot;ERCC-&quot;, rownames(sceset)) 6.5 Reading the data (10X) Due to the large size and sparsity of 10X data (upto 90% of the expression matrix may be 0s) it is typically stored as a sparse matrix. The default output format for CellRanger is an .mtx file which stores this sparse matrix as a column of row coordinates, a column of column corodinates, and a column of expression values &gt; 0. Note if you look at the .mtx file you will see two header lines followed by a line detailing the total number of rows, columns and counts for the full matrix. Since only the coordinates are stored in the .mtx file, the names of each row &amp; column must be stored separately in the “genes.tsv” and “barcodes.tsv” files respectively. We will be using the “Matrix” package to store matrices in sparse-matrix format in R. library(&quot;Matrix&quot;) ## ## 载入程辑包：&#39;Matrix&#39; ## The following object is masked from &#39;package:S4Vectors&#39;: ## ## expand cellbarcodes &lt;- read.table(&quot;data/droplet/Kidney-10X_P4_5/barcodes.tsv&quot;) genenames &lt;- read.table(&quot;data/droplet/Kidney-10X_P4_5/genes.tsv&quot;) molecules &lt;- readMM(&quot;data/droplet/Kidney-10X_P4_5/matrix.mtx&quot;) Now we will add the appropriate row and column names. However, if you inspect the read cellbarcodes you will see that they are just the barcode sequence associated with each cell. This is a problem since each batch of 10X data uses the same pool of barcodes so if we need to combine data from multiple 10X batches the cellbarcodes will not be unique. Hence we will attach the batch ID to each cell barcode: head(cellbarcodes) ## V1 ## 1 AAACCTGAGATGCCAG-1 ## 2 AAACCTGAGTGTCCAT-1 ## 3 AAACCTGCAAGGCTCC-1 ## 4 AAACCTGTCCTTGCCA-1 ## 5 AAACGGGAGCTGAACG-1 ## 6 AAACGGGCAGGACCCT-1 rownames(molecules) &lt;- genenames[,1] colnames(molecules) &lt;- paste(&quot;10X_P4_5&quot;, cellbarcodes[,1], sep=&quot;_&quot;) Now lets get the metadata and computational annotations for this data: meta &lt;- read.delim(&quot;data/droplet/droplet_metadata.csv&quot;, sep=&quot;,&quot;, header = TRUE) head(meta) ## channel mouse.id tissue subtissue mouse.sex ## 1 10X_P4_0 3-M-8 Tongue &lt;NA&gt; M ## 2 10X_P4_1 3-M-9 Tongue &lt;NA&gt; M ## 3 10X_P4_2 3-M-8/9 Liver hepatocytes M ## 4 10X_P4_3 3-M-8 Bladder &lt;NA&gt; M ## 5 10X_P4_4 3-M-9 Bladder &lt;NA&gt; M ## 6 10X_P4_5 3-M-8 Kidney &lt;NA&gt; M Here we can see that we need to use “10X_P4_5” to find the metadata for this batch, also note that the format of the mouse ID is different in this metadata table with hyphens instead of underscores and with the gender in the middle of the ID. From checking the methods section of the accompanying paper we know that the same 8 mice were used for both droplet and plate-based techniques. So we need to fix the mouse IDs to be consistent with those used in the FACS experiments. meta[meta$channel == &quot;10X_P4_5&quot;,] ## channel mouse.id tissue subtissue mouse.sex ## 6 10X_P4_5 3-M-8 Kidney &lt;NA&gt; M mouseID &lt;- &quot;3_8_M&quot; Note: depending on the tissue you have been assigned you may have 10X data from mixed samples : e.g. mouse id = 3-M-5/6. You should still reformat these to be consistent but they will not match mouse ids from the FACS data which may affect your downstream analysis. If the mice weren’t from an inbred strain it would be possible to assign individual cells to a specific mouse using exonic-SNPs but that is beyond the scope of this course. ann &lt;- read.delim(&quot;data/droplet/droplet_annotation.csv&quot;, sep=&quot;,&quot;, header=TRUE) head(ann) ## cell tissue cell_ontology_class ## 1 10X_P4_3_AAAGTAGAGATGCCAG Bladder mesenchymal cell ## 2 10X_P4_3_AACCGCGTCCAACCAA Bladder mesenchymal cell ## 3 10X_P4_3_AACTCCCGTCGGGTCT Bladder mesenchymal cell ## 4 10X_P4_3_AACTCTTAGTTGCAGG Bladder bladder cell ## 5 10X_P4_3_AACTCTTTCATAACCG Bladder mesenchymal cell ## 6 10X_P4_3_AAGACCTAGATCCGAG Bladder endothelial cell ## cell_ontology_term_iri cell_ontology_id ## 1 http://purl.obolibrary.org/obo/CL_0008019 CL:0008019 ## 2 http://purl.obolibrary.org/obo/CL_0008019 CL:0008019 ## 3 http://purl.obolibrary.org/obo/CL_0008019 CL:0008019 ## 4 http://purl.obolibrary.org/obo/CL_1001319 CL:1001319 ## 5 http://purl.obolibrary.org/obo/CL_0008019 CL:0008019 ## 6 http://purl.obolibrary.org/obo/CL_0000115 CL:0000115 Again you will find a slight formating difference between the cellID in the annotation and the cellbarcodes which we will have to correct before matching them. ann[,1] &lt;- paste(ann[,1], &quot;-1&quot;, sep=&quot;&quot;) ann_subset &lt;- ann[match(colnames(molecules), ann[,1]),] celltype &lt;- ann_subset[,3] Now lets build the cell-metadata dataframe: cell_anns &lt;- data.frame(mouse = rep(mouseID, times=ncol(molecules)), type=celltype) rownames(cell_anns) &lt;- colnames(molecules); Exercise Repeat the above for the other 10X batches for your tissue. Answer 6.6 Building a scater object Now that we have read the 10X data in multiple batches we need to combine them into a single SingleCellExperiment object. First we will check that the gene names are the same and in the same order across all batches: identical(rownames(molecules1), rownames(molecules2)) ## [1] TRUE identical(rownames(molecules1), rownames(molecules3)) ## [1] TRUE Now we’ll check that there aren’t any repeated cellIDs: sum(colnames(molecules1) %in% colnames(molecules2)) ## [1] 0 sum(colnames(molecules1) %in% colnames(molecules3)) ## [1] 0 sum(colnames(molecules2) %in% colnames(molecules3)) ## [1] 0 Everything is ok, so we can go ahead and combine them: all_molecules &lt;- cbind(molecules1, molecules2, molecules3) all_cell_anns &lt;- as.data.frame(rbind(cell_anns1, cell_anns2, cell_anns3)) all_cell_anns$batch &lt;- rep(c(&quot;10X_P4_5&quot;, &quot;10X_P4_6&quot;,&quot;10X_P7_5&quot;), times = c(nrow(cell_anns1), nrow(cell_anns2), nrow(cell_anns3))) Exercise How many cells are in the whole dataset? Answer Now build the SingleCellExperiment object. One of the advantages of the SingleCellExperiment class is that it is capable of storing data in normal matrix or sparse matrix format, as well as HDF5 format which allows large non-sparse matrices to be stored &amp; accessed on disk in an efficient manner rather than loading the whole thing into RAM. all_molecules &lt;- as.matrix(all_molecules) sceset &lt;- SingleCellExperiment( assays = list(counts = as.matrix(all_molecules)), colData = all_cell_anns ) Since this is 10X data it will not contain spike-ins, so we just save the data: saveRDS(sceset, &quot;kidney_droplet.rds&quot;) 6.7 Advanced Exercise Write an R function/script which will fully automate this procedure for each data-type for any tissue. "],
["references.html", "7 References", " 7 References "]
]
